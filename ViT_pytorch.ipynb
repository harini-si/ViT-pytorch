{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ViT-pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMHqgvzstihC/NbUuBlvyoE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harini-si/ViT-pytorch/blob/main/ViT_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjPRfmvUDpvs",
        "outputId": "a1472a3f-4d56-4015-8a54-d29e193594e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ViT-pytorch'...\n",
            "remote: Enumerating objects: 179, done.\u001b[K\n",
            "remote: Counting objects: 100% (49/49), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 179 (delta 37), reused 35 (delta 31), pack-reused 130\u001b[K\n",
            "Receiving objects: 100% (179/179), 21.31 MiB | 14.00 MiB/s, done.\n",
            "Resolving deltas: 100% (90/90), done.\n",
            "/content/ViT-pytorch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/harini-si/ViT-pytorch.git\n",
        "%cd ViT-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../\n",
        "!git clone https://github.com/NVIDIA/apex\n",
        "%cd apex\n",
        "!python setup.py install"
      ],
      "metadata": {
        "id": "UuEKLXH4DqX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ml_collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX13j5kdFd0s",
        "outputId": "49684f72-1e40-4d1d-f195-0a00924ba837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.0-py3-none-any.whl (88 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 23.9 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 20 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 30 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 40 kB 6.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 51 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 81 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 88 kB 3.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from ml_collections) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from ml_collections) (3.13)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.7/dist-packages (from ml_collections) (0.5.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from ml_collections) (0.12.0)\n",
            "Installing collected packages: ml-collections\n",
            "Successfully installed ml-collections-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../\n",
        "%cd ViT-pytorch\n",
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpXlXwwgFonv",
        "outputId": "578ebe95-1627-44de-e84a-dafe393e3ca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/ViT-pytorch\n",
            "--2022-01-11 09:50:53--  https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 108.177.97.128, 108.177.125.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412815506 (394M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B_16.npz’\n",
            "\n",
            "ViT-B_16.npz        100%[===================>] 393.69M  29.5MB/s    in 13s     \n",
            "\n",
            "2022-01-11 09:51:09 (29.5 MB/s) - ‘ViT-B_16.npz’ saved [412815506/412815506]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../\n",
        "!unzip \"/content/Interview_data.zip\" -d \"/content\""
      ],
      "metadata": {
        "id": "XsfsVn4Yar1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%cd ViT-pytorch\n",
        "!python3 train.py --name cifar10-100_500 --dataset my_data --model_type ViT-B_16 --pretrained_dir ViT-B_16.npz --gradient_accumulation_steps 5"
      ],
      "metadata": {
        "id": "Eee55mqsDt39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 train.py --name cifar10-100_500 --dataset my_data --model_type ViT-B_16 --pretrained_dir ViT-B_16.npz --fp16 --gradient_accumulation_steps 5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDYBNJPRk2FK",
        "outputId": "d8d76b0f-e395-478a-df30-43eee7c36dec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01/11/2022 11:20:41 - WARNING - __main__ - Process rank: -1, device: cuda, n_gpu: 1, distributed training: False, 16-bits training: True\n",
            "01/11/2022 11:20:46 - INFO - __main__ - classifier: token\n",
            "hidden_size: 768\n",
            "patches:\n",
            "  size: !!python/tuple [16, 16]\n",
            "representation_size: null\n",
            "transformer: {attention_dropout_rate: 0.0, dropout_rate: 0.1, mlp_dim: 3072, num_heads: 12,\n",
            "  num_layers: 12}\n",
            "\n",
            "01/11/2022 11:20:46 - INFO - __main__ - Training parameters Namespace(dataset='my_data', decay_type='cosine', device=device(type='cuda'), eval_batch_size=64, eval_every=1, fp16=True, fp16_opt_level='O2', gradient_accumulation_steps=5, img_size=224, learning_rate=0.05, local_rank=-1, loss_scale=0, max_grad_norm=1.0, model_type='ViT-B_16', n_gpu=1, name='cifar10-100_500', num_steps=10, output_dir='output', pretrained_dir='ViT-B_16.npz', seed=42, train_batch_size=64, warmup_steps=500, weight_decay=0)\n",
            "01/11/2022 11:20:46 - INFO - __main__ - Total Parameter: \t85.8M\n",
            "85.800194\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
            "\n",
            "Defaults for this optimization level are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Processing user overrides (additional kwargs that are not None)...\n",
            "After processing overrides, optimization options are:\n",
            "enabled                : True\n",
            "opt_level              : O2\n",
            "cast_model_type        : torch.float16\n",
            "patch_torch_functions  : False\n",
            "keep_batchnorm_fp32    : True\n",
            "master_weights         : True\n",
            "loss_scale             : dynamic\n",
            "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n",
            "01/11/2022 11:20:48 - INFO - __main__ - ***** Running training *****\n",
            "01/11/2022 11:20:48 - INFO - __main__ -   Total optimization steps = 10\n",
            "01/11/2022 11:20:48 - INFO - __main__ -   Instantaneous batch size per GPU = 12\n",
            "01/11/2022 11:20:48 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 60\n",
            "01/11/2022 11:20:48 - INFO - __main__ -   Gradient Accumulation steps = 5\n",
            "Training (X / X Steps) (loss=X.X):   0% 0/21 [00:00<?, ?it/s]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):   5% 1/21 [00:02<00:49,  2.47s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  10% 2/21 [00:03<00:35,  1.89s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  14% 3/21 [00:05<00:31,  1.73s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  19% 4/21 [00:07<00:28,  1.65s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:125: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 262144.0\n",
            "Training (1 / 10 Steps) (loss=0.69336):  19% 4/21 [00:08<00:28,  1.65s/it]/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "01/11/2022 11:20:57 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:20:57 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:20:57 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69315):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69315):  33% 1/3 [00:03<00:07,  3.85s/it]\u001b[A\n",
            "Validating... (loss=0.69315):  33% 1/3 [00:06<00:07,  3.85s/it]\u001b[A\n",
            "Validating... (loss=0.69315):  67% 2/3 [00:06<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.69315):  67% 2/3 [00:07<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.69315): 100% 3/3 [00:07<00:00,  2.51s/it]\n",
            "01/11/2022 11:21:05 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:21:05 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:21:05 - INFO - __main__ - Global Steps: 1\n",
            "01/11/2022 11:21:05 - INFO - __main__ - Valid Loss: 0.69315\n",
            "01/11/2022 11:21:05 - INFO - __main__ - Valid Accuracy: 0.45752\n",
            "01/11/2022 11:21:05 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (1 / 10 Steps) (loss=0.69336):  24% 5/21 [00:16<01:14,  4.65s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (1 / 10 Steps) (loss=0.69336):  29% 6/21 [00:18<00:53,  3.58s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (1 / 10 Steps) (loss=0.69336):  33% 7/21 [00:20<00:40,  2.91s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (1 / 10 Steps) (loss=0.69336):  38% 8/21 [00:21<00:32,  2.47s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (1 / 10 Steps) (loss=0.69336):  43% 9/21 [00:23<00:26,  2.17s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6934, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (2 / 10 Steps) (loss=0.69336):  43% 9/21 [00:24<00:26,  2.17s/it]01/11/2022 11:21:13 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:21:13 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:21:13 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69292):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69292):  33% 1/3 [00:03<00:07,  3.76s/it]\u001b[A\n",
            "Validating... (loss=0.69298):  33% 1/3 [00:06<00:07,  3.76s/it]\u001b[A\n",
            "Validating... (loss=0.69298):  67% 2/3 [00:06<00:03,  3.07s/it]\u001b[A\n",
            "Validating... (loss=0.69299):  67% 2/3 [00:07<00:03,  3.07s/it]\u001b[A\n",
            "Validating... (loss=0.69299): 100% 3/3 [00:07<00:00,  2.48s/it]\n",
            "01/11/2022 11:21:20 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:21:20 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:21:20 - INFO - __main__ - Global Steps: 2\n",
            "01/11/2022 11:21:20 - INFO - __main__ - Valid Loss: 0.69296\n",
            "01/11/2022 11:21:20 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "01/11/2022 11:21:21 - INFO - __main__ - Saved model checkpoint to [DIR: output]\n",
            "Training (2 / 10 Steps) (loss=0.69336):  48% 10/21 [00:32<00:50,  4.55s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6929, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (2 / 10 Steps) (loss=0.69336):  52% 11/21 [00:34<00:36,  3.62s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6929, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (2 / 10 Steps) (loss=0.69336):  57% 12/21 [00:36<00:26,  2.99s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6929, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (2 / 10 Steps) (loss=0.69336):  62% 13/21 [00:37<00:20,  2.55s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6929, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (2 / 10 Steps) (loss=0.69336):  67% 14/21 [00:39<00:15,  2.24s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6929, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (3 / 10 Steps) (loss=0.69287):  67% 14/21 [00:40<00:15,  2.24s/it]01/11/2022 11:21:29 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:21:29 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:21:29 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69235):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69235):  33% 1/3 [00:03<00:07,  3.80s/it]\u001b[A\n",
            "Validating... (loss=0.69249):  33% 1/3 [00:06<00:07,  3.80s/it]\u001b[A\n",
            "Validating... (loss=0.69249):  67% 2/3 [00:06<00:03,  3.09s/it]\u001b[A\n",
            "Validating... (loss=0.69253):  67% 2/3 [00:07<00:03,  3.09s/it]\u001b[A\n",
            "Validating... (loss=0.69253): 100% 3/3 [00:07<00:00,  2.50s/it]\n",
            "01/11/2022 11:21:37 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:21:37 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:21:37 - INFO - __main__ - Global Steps: 3\n",
            "01/11/2022 11:21:37 - INFO - __main__ - Valid Loss: 0.69245\n",
            "01/11/2022 11:21:37 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "Training (3 / 10 Steps) (loss=0.69287):  71% 15/21 [00:48<00:25,  4.30s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6924, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (3 / 10 Steps) (loss=0.69287):  76% 16/21 [00:49<00:17,  3.46s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6924, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (3 / 10 Steps) (loss=0.69287):  81% 17/21 [00:51<00:11,  2.88s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6924, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (3 / 10 Steps) (loss=0.69287):  86% 18/21 [00:52<00:07,  2.47s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6924, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (3 / 10 Steps) (loss=0.69287):  90% 19/21 [00:54<00:04,  2.19s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6924, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (4 / 10 Steps) (loss=0.69238):  90% 19/21 [00:55<00:04,  2.19s/it]01/11/2022 11:21:44 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:21:44 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:21:44 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69117):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.69117):  33% 1/3 [00:03<00:07,  3.79s/it]\u001b[A\n",
            "Validating... (loss=0.69162):  33% 1/3 [00:06<00:07,  3.79s/it]\u001b[A\n",
            "Validating... (loss=0.69162):  67% 2/3 [00:06<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.69173):  67% 2/3 [00:07<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.69173): 100% 3/3 [00:07<00:00,  2.51s/it]\n",
            "01/11/2022 11:21:52 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:21:52 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:21:52 - INFO - __main__ - Global Steps: 4\n",
            "01/11/2022 11:21:52 - INFO - __main__ - Valid Loss: 0.69151\n",
            "01/11/2022 11:21:52 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (4 / 10 Steps) (loss=0.69238):  95% 20/21 [01:03<00:04,  4.25s/it]torch.Size([4, 3, 224, 224])\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (4 / 10 Steps) (loss=0.69238): 100% 21/21 [01:03<00:00,  3.05s/it]\n",
            "Training (X / X Steps) (loss=X.X):   0% 0/21 [00:00<?, ?it/s]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):   5% 1/21 [00:02<00:42,  2.13s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  10% 2/21 [00:03<00:33,  1.78s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6909, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  14% 3/21 [00:05<00:30,  1.67s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  19% 4/21 [00:06<00:27,  1.61s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6914, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (5 / 10 Steps) (loss=0.69141):  19% 4/21 [00:08<00:27,  1.61s/it]01/11/2022 11:22:01 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:22:01 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:22:01 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68951):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68951):  33% 1/3 [00:03<00:07,  3.84s/it]\u001b[A\n",
            "Validating... (loss=0.69012):  33% 1/3 [00:06<00:07,  3.84s/it]\u001b[A\n",
            "Validating... (loss=0.69012):  67% 2/3 [00:06<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.69030):  67% 2/3 [00:07<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.69030): 100% 3/3 [00:07<00:00,  2.52s/it]\n",
            "01/11/2022 11:22:08 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:22:08 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:22:08 - INFO - __main__ - Global Steps: 5\n",
            "01/11/2022 11:22:08 - INFO - __main__ - Valid Loss: 0.68998\n",
            "01/11/2022 11:22:08 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "Training (5 / 10 Steps) (loss=0.69141):  24% 5/21 [00:15<01:09,  4.32s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6904, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (5 / 10 Steps) (loss=0.69141):  29% 6/21 [00:17<00:50,  3.36s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6895, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (5 / 10 Steps) (loss=0.69141):  33% 7/21 [00:18<00:38,  2.76s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6895, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (5 / 10 Steps) (loss=0.69141):  38% 8/21 [00:20<00:30,  2.37s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6895, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (5 / 10 Steps) (loss=0.69141):  43% 9/21 [00:21<00:25,  2.11s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6899, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (6 / 10 Steps) (loss=0.68994):  43% 9/21 [00:23<00:25,  2.11s/it]01/11/2022 11:22:16 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:22:16 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:22:16 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68709):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68709):  33% 1/3 [00:03<00:07,  3.84s/it]\u001b[A\n",
            "Validating... (loss=0.68798):  33% 1/3 [00:06<00:07,  3.84s/it]\u001b[A\n",
            "Validating... (loss=0.68798):  67% 2/3 [00:06<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.68828):  67% 2/3 [00:07<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.68828): 100% 3/3 [00:07<00:00,  2.52s/it]\n",
            "01/11/2022 11:22:23 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:22:23 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:22:23 - INFO - __main__ - Global Steps: 6\n",
            "01/11/2022 11:22:23 - INFO - __main__ - Valid Loss: 0.68778\n",
            "01/11/2022 11:22:23 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "Training (6 / 10 Steps) (loss=0.68994):  48% 10/21 [00:31<00:47,  4.28s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6865, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (6 / 10 Steps) (loss=0.68994):  52% 11/21 [00:32<00:34,  3.44s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6875, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (6 / 10 Steps) (loss=0.68994):  57% 12/21 [00:34<00:25,  2.85s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6875, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (6 / 10 Steps) (loss=0.68994):  62% 13/21 [00:35<00:19,  2.46s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6865, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (6 / 10 Steps) (loss=0.68994):  67% 14/21 [00:37<00:15,  2.18s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6890, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (7 / 10 Steps) (loss=0.68896):  67% 14/21 [00:38<00:15,  2.18s/it]01/11/2022 11:22:31 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:22:31 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:22:31 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68374):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.68374):  33% 1/3 [00:03<00:07,  3.77s/it]\u001b[A\n",
            "Validating... (loss=0.68523):  33% 1/3 [00:06<00:07,  3.77s/it]\u001b[A\n",
            "Validating... (loss=0.68523):  67% 2/3 [00:06<00:03,  3.09s/it]\u001b[A\n",
            "Validating... (loss=0.68570):  67% 2/3 [00:07<00:03,  3.09s/it]\u001b[A\n",
            "Validating... (loss=0.68570): 100% 3/3 [00:07<00:00,  2.50s/it]\n",
            "01/11/2022 11:22:39 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:22:39 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:22:39 - INFO - __main__ - Global Steps: 7\n",
            "01/11/2022 11:22:39 - INFO - __main__ - Valid Loss: 0.68489\n",
            "01/11/2022 11:22:39 - INFO - __main__ - Valid Accuracy: 0.96732\n",
            "Training (7 / 10 Steps) (loss=0.68896):  71% 15/21 [00:46<00:25,  4.25s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6841, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (7 / 10 Steps) (loss=0.68896):  76% 16/21 [00:47<00:17,  3.43s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6841, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (7 / 10 Steps) (loss=0.68896):  81% 17/21 [00:49<00:11,  2.86s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6836, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (7 / 10 Steps) (loss=0.68896):  86% 18/21 [00:50<00:07,  2.46s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6855, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (7 / 10 Steps) (loss=0.68896):  90% 19/21 [00:52<00:04,  2.18s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6851, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (8 / 10 Steps) (loss=0.68506):  90% 19/21 [00:53<00:04,  2.18s/it]01/11/2022 11:22:46 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:22:46 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:22:46 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67921):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67921):  33% 1/3 [00:03<00:07,  3.78s/it]\u001b[A\n",
            "Validating... (loss=0.68196):  33% 1/3 [00:06<00:07,  3.78s/it]\u001b[A\n",
            "Validating... (loss=0.68196):  67% 2/3 [00:06<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.68272):  67% 2/3 [00:07<00:03,  3.11s/it]\u001b[A\n",
            "Validating... (loss=0.68272): 100% 3/3 [00:07<00:00,  2.51s/it]\n",
            "01/11/2022 11:22:54 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:22:54 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:22:54 - INFO - __main__ - Global Steps: 8\n",
            "01/11/2022 11:22:54 - INFO - __main__ - Valid Loss: 0.68129\n",
            "01/11/2022 11:22:54 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (8 / 10 Steps) (loss=0.68506):  95% 20/21 [01:01<00:04,  4.26s/it]torch.Size([4, 3, 224, 224])\n",
            "tensor(0.6782, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (8 / 10 Steps) (loss=0.68506): 100% 21/21 [01:02<00:00,  2.96s/it]\n",
            "Training (X / X Steps) (loss=X.X):   0% 0/21 [00:00<?, ?it/s]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6807, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):   5% 1/21 [00:02<00:41,  2.09s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6792, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  10% 2/21 [00:03<00:33,  1.76s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6797, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  14% 3/21 [00:05<00:29,  1.66s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6812, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (X / X Steps) (loss=X.X):  19% 4/21 [00:06<00:27,  1.61s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6816, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (9 / 10 Steps) (loss=0.68164):  19% 4/21 [00:08<00:27,  1.61s/it]01/11/2022 11:23:03 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:23:03 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:23:03 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67365):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.67365):  33% 1/3 [00:03<00:07,  3.79s/it]\u001b[A\n",
            "Validating... (loss=0.67796):  33% 1/3 [00:06<00:07,  3.79s/it]\u001b[A\n",
            "Validating... (loss=0.67796):  67% 2/3 [00:06<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.67907):  67% 2/3 [00:07<00:03,  3.12s/it]\u001b[A\n",
            "Validating... (loss=0.67907): 100% 3/3 [00:07<00:00,  2.52s/it]\n",
            "01/11/2022 11:23:10 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:23:10 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:23:10 - INFO - __main__ - Global Steps: 9\n",
            "01/11/2022 11:23:10 - INFO - __main__ - Valid Loss: 0.67689\n",
            "01/11/2022 11:23:10 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (9 / 10 Steps) (loss=0.68164):  24% 5/21 [00:15<01:09,  4.32s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6758, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (9 / 10 Steps) (loss=0.68164):  29% 6/21 [00:17<00:50,  3.37s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6743, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (9 / 10 Steps) (loss=0.68164):  33% 7/21 [00:18<00:38,  2.77s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6748, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (9 / 10 Steps) (loss=0.68164):  38% 8/21 [00:20<00:30,  2.38s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6729, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (9 / 10 Steps) (loss=0.68164):  43% 9/21 [00:21<00:25,  2.11s/it]torch.Size([12, 3, 224, 224])\n",
            "tensor(0.6777, device='cuda:0', grad_fn=<ToCopyBackward0>)\n",
            "Training (10 / 10 Steps) (loss=0.67773):  43% 9/21 [00:23<00:25,  2.11s/it]01/11/2022 11:23:18 - INFO - __main__ - ***** Running Validation *****\n",
            "01/11/2022 11:23:18 - INFO - __main__ -   Num steps = 3\n",
            "01/11/2022 11:23:18 - INFO - __main__ -   Batch size = 64\n",
            "\n",
            "Validating... (loss=X.X):   0% 0/3 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.66707):   0% 0/3 [00:03<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.66707):  33% 1/3 [00:03<00:07,  3.86s/it]\u001b[A\n",
            "Validating... (loss=0.67317):  33% 1/3 [00:06<00:07,  3.86s/it]\u001b[A\n",
            "Validating... (loss=0.67317):  67% 2/3 [00:06<00:03,  3.14s/it]\u001b[A\n",
            "Validating... (loss=0.67467):  67% 2/3 [00:07<00:03,  3.14s/it]\u001b[A\n",
            "Validating... (loss=0.67467): 100% 3/3 [00:07<00:00,  2.53s/it]\n",
            "01/11/2022 11:23:26 - INFO - __main__ - \n",
            "\n",
            "01/11/2022 11:23:26 - INFO - __main__ - Validation Results\n",
            "01/11/2022 11:23:26 - INFO - __main__ - Global Steps: 10\n",
            "01/11/2022 11:23:26 - INFO - __main__ - Valid Loss: 0.67163\n",
            "01/11/2022 11:23:26 - INFO - __main__ - Valid Accuracy: 0.97386\n",
            "Training (10 / 10 Steps) (loss=0.67773):  43% 9/21 [00:31<00:41,  3.47s/it]\n",
            "01/11/2022 11:23:26 - INFO - __main__ - Best Accuracy: \t0.973856\n",
            "01/11/2022 11:23:26 - INFO - __main__ - End Training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader,ConcatDataset\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import random"
      ],
      "metadata": {
        "id": "4DiQhAHpoQMO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}